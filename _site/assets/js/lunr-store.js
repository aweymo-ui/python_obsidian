var store = [ 
    
    
    { 
        "url": "/content/1_intro.html",
        "title": "Introduction",
        "text": "Hello, my name is Andrew Weymouth and I have worked with the University of Idaho Library as the Digital Initiatives Librarian in the Center for Digital Inquiry and Learning (CDIL) department since the fall of 2023. My work generally consists of creating and maintaining our digital collections, working with CDIL fellowship recipients, helping to rethink processes and introducing new digital scholarship tools to the department. Three of Many Other Books the Author Needs to Read Why? This work was completely inspired out of necessity for a master’s in History with the University of Idaho in the fall of 2024 while also working as a faculty member with the library. I wanted to think about creating a note taking system that could visualize connections across many different books that would be beneficial for creating historiographies specifically, but would also be helpful while creating literature reviews, scientific reviews and environmental scans. &#10042; Note: Everything that I will be talking about today is completely open source or available to you at no cost as a University of Idaho student or staff member."
    },
    { 
        "url": "/content/2_accounts.html",
        "title": "Accounts",
        "text": "Obsidian Graph View. Obsidian was founded in 2020 as a markdown file based knowledge management tool that has a very active user base and regularly updates systems based on that feedback. Obsidian supports the use of multiple computer languages, including JavaScript, HTML and CSS but, as we will see later, you can do as much or as little of this kind of thing on the pages depending on your experience with / interest in these technical approaches. Most of the tagging we will be focusing on for this presentation involves memorizing two or three characters to use Obsidian’s most powerful tools, the visualization or “graph view”, which is a very helpful guide to understanding the similarities and differences across a diverse collection of reading material. Obsidian To create an Obsidian account, you will first need to download the application which is supported in Windows, Mac, Linux, IOS and Android. Afterwards, you will create an account. Note that Obsidian does have a paid Sync and Publish features but you don’t need either for everything we will cover in this presentation. Each workspace you create in Obsidian is a folder that they call a vault. &#10042; You can create a syncing vault simply by creating a folder in a cloud based storage such as OneDrive or Google Drive, choosing Open Vault in the Obsidian file drop down and selecting that folder. Note that whichever cloud based storage you choose may dictate your accessibility on other devices. For example, because I chose my University of Idaho OneDrive for a vault I’ll be talking about today, I can access this vault on the Android app due to OIT privacy standards but I can access it on all of my various laptop and desktops, which works for the capacity that I usually need these notes. Obsidian Vault Page. Considering Folder Structures Another thing to consider is the scope of your vault when you are creating it. When I first started experimenting with Obsidian, I was using it essentially like a Google Drive with many different Google Sheets in each folder and my scope was not defined, which isn’t really what Obsidian excels at. &#10042; Although there are ways to collaborate on a vault using either a shared cloud space or a GitHub repository (both not without potential syncing issues) Obsidian is really designed as a personal knowledge management tool and it works best when you are analyzing a somewhat limited pool of documents. This may be a single project, class or academic discipline that you want to compare and interconnect in different ways. That said, since a vault is essentially just a folder, you can easily add or remove different items to change the scope of your analytical focus at any point. Connecting GitHub, GitHub Desktop and Visual Studio Code GitHub Registration Page Now that we have our Obsidian account and a vault structure, you will need to create an account with GitHub a cloud-based platform that allows developers to store, share and collaborate on code. After you have an account, download GitHub Desktop and, with your Git web browser open, sign through Preferences &gt; Accounts, which will prompt your browser to verify. Connecting GitHub Desktop to Git through Preferences Think of GitHub Desktop as a bridge between your GitHub’s web browser and your local Visual Studio Code that is saving you a little coding that you would otherwise need to do from the command line. GitHub Desktop button that will prompt verification through the Web Browser Next download a text editor. A simple, no cost option that most of us use in the Center for Digital Inquiry and Learning is Visual Studio Code. Signing in through the settings widget on the bottom left of the interface will prompt the same verification of your GitHub account through the web browser. Visual Studio Code prompting verification through the GitHub Web Browser Now that these elements are all connected, you should be able to clone either the Annotation Extraction Tool or the book splitter Python tools by pushing the green Code button and selecting Open with GitHub Desktop. GitHub Browser view of the option to clone Python repositories through GitHub Desktop In GitHub Desktop, select Fetch Origin and then select Open in Visual Studio Code to begin! GitHub Desktop Interface"
    },
    { 
        "url": "/content/3_process.html",
        "title": "Process",
        "text": "Contents: Overview | Transcription | Python Text Mining | Linking Primary Tag Sheet to Individual Transcripts | Copyediting Workflow Visualization Overview To summarize the process: Audio is transcribed into CSV files by Premiere CSVs are made into individual Google Sheets and also added to the Python Transcription Mining Tool Within the tool, these items are concatenated and searched for all associated words and phrases built into the tool under different tag categories The tool generates a tally of the these words and phrases, which is used to create the “Primary Tag Sheet” in another Google Sheet Using the Apps Script function, all individual transcripts are linked to the primary tag sheet so their tag fields are automatically generated New categories or associated words can be added or removed to the Primary Tag Sheet and these changes can be implemented across all individual transcripts by simply re-running the code Individual changes can be implemented during the student worker led copy editing process to catch any data driven errors &#10042; Transcription Moving away from the Otter.AI the department had been working with, I tested Adobe Premiere’s transcription tools and found it uniquely well-suited for the OHD framework with advantages including dramatically increased accuracy in differentiating speakers and transcribing dialogue, significantly faster transcription speed, from one hour long recording every two to three business days up to twenty hour recordings a day and high privacy standards with GDPR compliance, ensuring all transcription material is stored locally and not uploaded to the cloud.[1] Example of transcript CSV formatting as it exports from Adobe Premiere That said, the tool is not perfect. While modern recordings in good conditions have extremely high transcription accuracy, poor quality recordings or interviews between two similar sounding people can require significant copyediting. Recent work by the Matt Miller of the Library of Congress has me very interested in creating custom speech to text tools using Whisper(.cpp) to possibly help improve on these inaccuracies.[2] &#10042; Python Text Mining After initial tests using the web based text mining tool Voyant, I wanted to create a text mining tool from scratch using Python that would allow me to identify specific words and phrases, create custom tagging categories and “stopwords”(words removed from text before processing and analysis) for each collection. Once the CSVs of the transcript are added to a folder in the Python workspace, the code imports text mining libraries such as Pandas, NLTK, TextBlob, and regular expressions for processing. Extract of the Python code in the Geographic&gt;Finland section of the text mining tool Below this header material in the Python file are three categories of tags: general, geographic and custom, which contain twenty subsections each and these subsections contain 50 associated words or phrases that are being identified throughout the combined corpus and ultimately tallied to produce the output shown here: &#10042; Example of Text Mining Tool Output for the Rural Women's History Project See Appendix 1 for an excerpt of the this script or visit the Git to view in full. Linking Primary Tag Sheet to Individual Transcripts This data is then entered into a “primary tag sheet” also in Google Sheets. After formatting their transcripts, the student worker opens the Apps Script extension and enters a string of code (see Appendix 2). After making two minor adjustments based on the URL of their transcript, it will be linked to the Primary Tag Sheet. &#10042; Copyediting This process is not intended to replace human transcribers, instead this aims to enhance the role of human transcribers by shifting the focus from manual tagging to copy editing, thus reducing repetitive tasks. Student workers are encouraged to modify tag names, adjust associated words, and create new categories based on trends identified in the primary tag sheet. This approach not only diversifies the tasks of student workers beyond transcription—often monotonous, with little to highlight on a CV—and enables them to engage in coding and instantly see the impact of their modifications. Other advantages include: All tags use a controlled vocabulary. Tagging is more accurate, detailed, and relevant, helping researchers quickly identify thematic connections. Tagging establishes a knowledge framework relevant to the collection that transcribers might lack in historical, scientific, or regional contexts key to the recordings."
    },
    { 
        "url": "/content/4_findings.html",
        "title": "Findings and Conclusion",
        "text": "Pre and Post Process Tagging Visualization Regarding the limitations of data-driven, human-edited tagging, program managers should clarify that automated tags are only a starting point and may be incorrectly applied, missing, or require broader application across transcripts. Also, the amount of detail this process accrues is drastic and one could argue that the density of the data now makes it difficult for the researcher to navigate, especially on mobile devices, which remains a topic for ongoing refinement. My hope is that utilizing machine learning, Python, and JavaScript approaches will make digitizing these resources more efficient and accessible, promoting their preservation and availability to the public."
    },
    { 
        "url": "/content/5_references_apendices.html",
        "title": null,
        "text": "Contents: Notes | Appendix 1. Excerpt of Python Text Mining Tool | Appendix 2. Apps Script Example for Linking Transcript to Primary Tag Sheet | About the Author &#10042; Notes [1] Speech to Text in Premiere Pro FAQ. Adobe; [cited 2024 Jul 8]. Available from: https://helpx.adobe.com/content/help/en/premiere-pro/using/speech-to-text-faq.html [2] Matt Miller. “Lomax Whisper,” September 12, 2024. https://thisismattmiller.com/post/lomax-whisper/ Appendix 1. Excerpt of Python Text Mining Tool import pandas as pd import string from nltk.corpus import stopwords from collections import Counter import re from textblob import TextBlob Download NLTK stopwords data import nltk nltk.download('stopwords') Define preprocess_text function def preprocess_text(text): if isinstance(text, str): # Check if text is a string text = re.sub(r'\\b\\w{1,4}\\b', '', text) # Remove short words (length &lt;= 4) text = text.translate(str.maketrans('', '', string.punctuation)) text = text.lower() # Convert text to lowercase else: text = '' # Replace NaNs with an empty string return text Load stopwords for both Spanish and English stop_words_spanish = set(stopwords.words('spanish')) stop_words_english = set(stopwords.words('english')) Combine both sets of stopwords stop_words = stop_words_spanish.union(stop_words_english) import os Directory containing CSV files directory = 'C:\\\\Users\\\\aweymouth\\\\Documents\\\\Github\\\\transcript_mining_base\\\\CSV' List of CSV file names file_names = [ 'rwhp070.csv', 'rwhp075.csv', 'rwhp079.csv', 'rwhp083.csv', 'rwhp088.csv', 'rwhp109.csv', 'rwhp123.csv', 'rwhp174.csv', 'rwhp225.csv', 'rwhp261.csv', 'rwhp277.csv', 'rwhp277.csv', 'rwhp297.csv', 'rwhp320.csv', 'rwhp323.csv', 'rwhp378.csv', 'rwhp385.csv', 'rwhp410.csv', 'rwhp418.csv', 'rwhp420.csv', 'rwhp421.csv', 'rwhp422.csv', 'rwhp425.csv', 'rwhp426.csv', 'rwhp427.csv' ] Construct file paths using os.path.join() file_paths = [os.path.join(directory, file_name) for file_name in file_names] dfs = [pd.read_csv(file_path, encoding='utf-8') for file_path in file_paths] Concatenate text data from all dataframes into a single corpus corpus = '' for df in dfs: text_series = df['text'].fillna('').astype(str).str.lower().str.strip() # Extract and preprocess text column corpus += ' '.join(text_series) + ' ' # Concatenate preprocessed text with space delimiter Preprocess the entire corpus cleaned_corpus = preprocess_text(corpus) Remove stopwords from the corpus filtered_words = [word for word in cleaned_corpus.split() if word not in stop_words and len(word) &gt;= 5] Count the frequency of each word word_freq = Counter(filtered_words) Get top 100 most frequent distinctive words with occurrences top_distinctive_words = word_freq.most_common(100) === General Section === from collections import Counter import re def find_agriculture_terms(corpus): # Define a list of agriculture-related terms agriculture_terms = [\"harvest\", \"tractor\", \"acreage\", \"crop\", \"livestock\", \"farm field\", \"barn building\", \"ranch\", \"garden\", \"orchard\", \"dairy\", \"cattle\", \"poultry\", \"equipment\", \"fertilizer\", \"seed\", \"irrigation\", \"plow\", \"farmhand\", \"hoe\", \"shovel\", \"milking\", \"hay\", \"silage\", \"compost\", \"weeding\", \"crop rotation\", \"organic\", \"gmo\", \"sustainable\", \"farming\", \"rural\", \"homestead\", \"tilling\", \"wheat\", \"corn maize\", \"soybean\", \"potato\", \"apple fruit\", \"berry\", \"honey\", \"apiary\", \"pasture\", \"combine harvester\", \"trailer\", \"baler\", \"thresher\"] # Initialize a Counter to tally occurrences of agriculture-related terms agriculture_word_freq = Counter() # Tokenize the corpus to handle multi-word expressions tokens = re.findall(r'\\b\\w+\\b', corpus.lower()) # Iterate over each token in the corpus for word in tokens: # Check if the token is an agriculture-related term if word in agriculture_terms: agriculture_word_freq[word] += 1 # Return the top 50 most common agriculture-related terms return agriculture_word_freq.most_common(50) Call the function to find agriculture-related terms in the corpus top_agriculture_terms = find_agriculture_terms(corpus) Print the top 50 agriculture-related terms print(\"## agriculture\") for word, count in top_agriculture_terms: print(f\"{word.capitalize()}: {count}\") Appendix 2. Apps Script Example for Linking Transcript to Primary Tag Sheet function fillTags() { // Get the active spreadsheet var spreadsheet = SpreadsheetApp.getActiveSpreadsheet(); // Get the transcript sheet by name var transcriptSheet = spreadsheet.getSheetByName(\"Callison3\"); if (!transcriptSheet) { Logger.log(\"Transcript sheet not found\"); return; } // Set the header in cell E1 to \"tags\" transcriptSheet.getRange(\"E1\").setValue(\"tags\"); // Get the tags spreadsheet by URL var tagsSpreadsheet = SpreadsheetApp.openByUrl(\"https://docs.google.com/spreadsheets/d/1soOfgdAjik_TL8WX9dDV9BaFb1RQJc8_BVu7sBGnNUE/edit?gid=419710039#gid=419710039\"); if (!tagsSpreadsheet) { Logger.log(\"Tags spreadsheet not found\"); return; } // Get the tags sheet within the tags spreadsheet var tagsSheet = tagsSpreadsheet.getSheetByName(\"tags\"); if (!tagsSheet) { Logger.log(\"Tags sheet not found\"); return; } // Get the range of the transcript column var transcriptRange = transcriptSheet.getRange(\"D2:D\" + transcriptSheet.getLastRow()); var transcriptValues = transcriptRange.getValues(); // Get the range of example words and tags in the tags sheet var exampleWordsRange = tagsSheet.getRange(\"B2:B\" + tagsSheet.getLastRow()); var tagsRange = tagsSheet.getRange(\"A2:A\" + tagsSheet.getLastRow()); var exampleWordsValues = exampleWordsRange.getValues(); var tagsValues = tagsRange.getValues(); // Create a map of example words to tags var tagsMap = {}; for (var i = 0; i &lt; exampleWordsValues.length; i++) { var word = exampleWordsValues[i][0].toLowerCase(); var tag = tagsValues[i][0]; tagsMap[word] = tag; } // Initialize an array to store the tags for each transcript entry var transcriptTags = []; // Loop through each transcript entry for (var i = 0; i &lt; transcriptValues.length; i++) { var text = transcriptValues[i][0]; var uniqueTags = []; if (typeof text === 'string') { // Use regular expression to extract words and handle punctuation var words = text.match(/\\b\\w+['-]?\\w*|\\w+['-]?\\w*\\b/g); // Check each word in the transcript entry against the tags map if (words) { for (var j = 0; j &lt; words.length; j++) { var word = words[j].toLowerCase().replace(/[.,!?;:()]/g, ''); var singularWord = word.endsWith('s') ? word.slice(0, -1) : word; if (tagsMap.hasOwnProperty(word) &amp;&amp; !uniqueTags.includes(tagsMap[word])) { uniqueTags.push(tagsMap[word]); } else if (tagsMap.hasOwnProperty(singularWord) &amp;&amp; !uniqueTags.includes(tagsMap[singularWord])) { uniqueTags.push(tagsMap[singularWord]); } } } } // Add the determined tags to the array transcriptTags.push([uniqueTags.join(\";\")]); } // Get the range of the tags column in the transcript sheet, starting from E2 var tagsColumn = transcriptSheet.getRange(\"E2:E\" + (transcriptTags.length + 1)); // Set the values in the tags column to the determined tags tagsColumn.setValues(transcriptTags); } About the Author Andrew Weymouth is the Digital Initiatives Librarian at University of Idaho, specializing in static web design to curate the institution’s special collections and partner with faculty and students on fellowship projects. His work spans digital scholarship projects at the universities of Oregon and Washington and the Tacoma Northwest Room archives, including long form audio public history projects, architectural databases, oral history and network visualizations. He writes about labor, architecture, underrepresented communities and using digital methods to survey equity in archival collections."
    },
    { 
        "url": "/",
        "title": "Home",
        "text": "Slides Git Repository for Transcript Text Mining Tool Overview This presentation will provide a walkthrough of new processes for creating subject tags across complete oral history collections developed at the U of I Center for Digital Inquiry and Learning over the summer of 2024. It details a workflow enabling student workers to run, modify, and expand tags during copyediting, aiming to enhance accuracy and ultimately help researchers identify connections between recordings. We’ll cover the workflow, challenges it addresses and the limitations of data-driven, human-edited tagging. Contents: Introduction Accounts Process Findings References and Appendices Content: CC BY-NC-ND 4.0 Andrew Weymouth 2024 (get source code). Theme: Variation on workshop-template-b by evanwill"
    }];
